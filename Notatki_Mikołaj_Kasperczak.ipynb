{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KasperczakMikolaj/example-mikolaj3/blob/main/Notatki_Miko%C5%82aj_Kasperczak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75g9TwAIdVEa"
      },
      "outputs": [],
      "source": [
        "# Instalacja OpenJDK, pobieranie i rozpakowywanie Apache Spark, instalacja findspark i pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install -q pyspark==3.2.1\n",
        "\n",
        "# Ustawienie zmiennych środowiskowych\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "# Inicjalizacja findspark i pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Sprawdzenie, czy istnieje SparkContext, zanim utworzymy nowy\n",
        "if not SparkContext._active_spark_context:\n",
        "    sc = SparkContext('local[*]')\n",
        "else:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Inicjalizacja SparkSession\n",
        "spark = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD API. Wyświetl top 5 krajów pod względem liczby łącznych zgonów\n",
        " #(total_deaths) w roku 2020. Do każdej krotki dodaj na końcu napis\n",
        " #\"TOTAL DEATHS IN 2020\" UWAGA! W raporcie niektóre kraje są podzielone\n",
        " #na regiony (np. USA). W takich przypadkach musisz je zgrupować do poziomu\n",
        " # kraju.\n",
        "death_data = sc.textFile('/content/all_weekly_excess_deaths.csv')\n",
        "death_data.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igav5egde_o0",
        "outputId": "8b911372-17b8-435f-b95b-66c2577997e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['country;region;region_code;start_date;end_date;days;year;week;population;total_deaths;covid_deaths;expected_deaths;excess_deaths;non_covid_deaths;covid_deaths_per_100k;excess_deaths_per_100k;excess_deaths_pct_change',\n",
              " 'Australia;Australia;0;01/01/2020;07/01/2020;7;2020;1;25734100;2497 deaths;0;2463.11165730355;33.8883426964494;2497;0;0.131686527589655;0.0137583461131225',\n",
              " 'Australia;Australia;0;08/01/2020;14/01/2020;7;2020;2;25734100;2510 deaths;0;2458.27832397007;51.721676029927;2510;0;0.200984981133698;0.0210397966436924',\n",
              " 'Australia;Australia;0;15/01/2020;21/01/2020;7;2020;3;25734100;2501 deaths;0;2436.44499063674;64.5550093632601;2501;0;0.250853961721063;0.0264955743352897',\n",
              " 'Australia;Australia;0;22/01/2020;28/01/2020;7;2020;4;25734100;2597 deaths;0;2436.61165730341;160.388342696594;2597;0;0.623252193379967;0.065824335287838']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(DataFrame API) W jakim tygodniu w każdym z krajów nastąpił\n",
        "#największy przyrost liczby zgonów (total_deaths) względem liczby z\n",
        "#poprzedniego tygodnia? (Różnica między total_deaths w danym tygodniu\n",
        "#a tygodniu poprzedzającym) Wynik posortuj malejąco według różnic\n",
        "#i wyświetl top 5 krajów z pokazanymi tygodniami i wartościami tak jak w\n",
        "#przykładowym wyniku.\n",
        " (death_data\n",
        "  .map(lambda x: x.split(';'))\n",
        "  .filter(lambda x: 'country' not in x)\n",
        "  .map(lambda x:(x[0],x[6],x[9].split(' ')[0]))\n",
        "  .filter(lambda x: x[1] == '2020')\n",
        "  .map(lambda x: (x[0],int(x[2])))\n",
        "  .reduceByKey(lambda x,y:x+y)\n",
        "  ).top(5,key=lambda x:x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz0fXHYnfJf6",
        "outputId": "aadb3f95-a41e-4d7b-b0e1-7cc6b90d416a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('United States', 6964329),\n",
              " ('Britain', 2006607),\n",
              " ('Italy', 1468112),\n",
              " ('France', 1309358),\n",
              " ('Mexico', 1065198)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pokazywanie 5 pierwszych\n",
        "!head -5 (nazwa csv)"
      ],
      "metadata": {
        "id": "1e-AHngwgJ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wyświetl wartość poszczególnych branż (Industry) w Stanach Zjednoczonych\n",
        "#pod względem zsumowanego majątku najbogatszych ludzi pochodzących z tych\n",
        "#branż (pogrupuj branże po Total Net Worth). Dodaj na końcu kolumnę z\n",
        "# tekstem 'dollars'. Usuń pozycję która zawiera pusty tekst ''.\n",
        "\n",
        "rich = sc.textFile('500_richest.csv')\n",
        "\n",
        "rich_no_header = rich.filter(lambda x: x!='Name;Total Net Worth;$ Last Change;$ YTD Change;Country;Industry')\n",
        "\n",
        "rich_no_header= rich_no_header.map(lambda x: x.split(';'))\n",
        "rich_no_header.take(1)\n",
        "\n",
        "\n",
        "#tworzenieKrotek\n",
        "#zatąpienie znaku $\n",
        "rich_no_header = rich_no_header.map(lambda x: (x[5], x[1].replace('$', '')))\n",
        "#Zmina 1.8B na konkretną liczbe\n",
        "rich_no_header = rich_no_header.map(lambda x: (x[0], x[1].replace('B', '')))\n",
        "rich_no_header = rich_no_header.map(lambda x: (x[0], float(x[1]) *1000000000))\n",
        "\n",
        "#Usunięcie pustej wartości\n",
        "rich_no_header=rich_no_header.filter(lambda x: x[0])\n",
        "\n",
        "rich_no_header.take(10)\n",
        "\n",
        "(rich_no_header.map(lambda x: (x[0],int(x[1])))\n",
        "    .reduceByKey(lambda x,y:x+y)\n",
        "    .map(lambda x:(x[0],x[1],'dollars')).collect()\n",
        "\n",
        "   )\n"
      ],
      "metadata": {
        "id": "sFIWi2XgpftP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rozpatrzmy następujące kraje: Rosja, Szwecja, Niemcy\n",
        "#(kolumna Country: Russia, Sweden, Germany).\n",
        "#Wyświetl czwartego najbogatszego człowieka w każdym z tych krajów\n",
        "#oraz łączną wartość branży wśród najbogatszych ludzi\n",
        " #(suma Total Net Worth) w kraju w którym występuje ta osoba w rankingu.\n",
        " # Możesz pominąć BLN przy sumowaniu (Industry_Total_In_Country_BLN)\n",
        "from pyspark.sql.functions import *\n",
        "import sys\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "rich_2=spark.read.format('csv').option(\"header\", True).option(\"delimiter\", \";\").load('500_richest.csv')\n",
        "\n",
        "\n",
        "#Pozostawienie tylko kolumn na których bede pracować\n",
        "rich_2 =rich_2.select(\"Country\", \"Name\", 'Industry', 'Total Net Worth')\n",
        "\n",
        "#Usuwanie zbędnych znaków i rzutowanie na float\n",
        "\n",
        "rich_2 = rich_2.withColumn(\"Total Net Worth\", f.translate(f.col(\"Total Net Worth\"), \"$#,\", \"\").alias(\"replaced\"))\n",
        "\n",
        "rich_2 = rich_2.withColumn(\"Total Net Worth\", f.regexp_replace(\"Total Net Worth\", \"B\", \"\").cast(\"float\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rich_2.show()\n",
        "\n",
        "\n",
        "\n",
        "#grupowanie\n",
        "\n",
        "df_rich = (rich_2.groupBy(\"Country\",'Industry')\n",
        "            .agg(round(sum(\"Total Net Worth\"),2)\n",
        "            .alias(\"Industry_Total_In_Country_BLN\"), first('Name').alias(\"Name\")))\n",
        "df_rich.show()"
      ],
      "metadata": {
        "id": "kqC9VLimu0tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sortowanie danych według kolumny \"Industry_Total_In_Country_BLN\" malejąco\n",
        "df_rich = df_rich.orderBy(\"Industry_Total_In_Country_BLN\", ascending=False)\n",
        "\n",
        "# Definicja specyfikacji okna - grupowanie według kraju i sortowanie według \"Industry_Total_In_Country_BLN\" malejąco\n",
        "windowSpec = (Window\n",
        "              .partitionBy(f.col('Country'))\n",
        "              .orderBy(f.col(\"Industry_Total_In_Country_BLN\").desc())\n",
        "              )\n",
        "\n",
        "# Dodanie kolumny \"row_number\", która przypisuje numer wiersza w obrębie każdej grupy krajów\n",
        "new_df = df_rich.withColumn('row_number', row_number().over(windowSpec))\n",
        "\n",
        "# Filtrowanie wierszy, aby pozostawić tylko te, które mają \"row_number\" równy 4\n",
        "new_df = new_df.filter(f.col('row_number') == 4)\n",
        "\n",
        "# Wybieranie kolumn \"Country\", \"Name\", \"Industry\" oraz \"Industry_Total_In_Country_BLN\"\n",
        "new_df = new_df.select(\"Country\", \"Name\", 'Industry', 'Industry_Total_In_Country_BLN')\n",
        "\n",
        "# Konwersja wynikowego DataFrame do Pandas DataFrame\n",
        "final_answer = new_df.toPandas()\n",
        "\n",
        "# Wyświetlanie wynikowego DataFrame\n",
        "new_df.show()\n",
        "\n",
        "# Zwrócenie wynikowego Pandas DataFrame\n",
        "final_answer"
      ],
      "metadata": {
        "id": "qHv9Rop9vf-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Załaduj plik pracownicy.csv z poprawnie zdefiniowaną strukturą i\n",
        "#typami danych (schema).#Dla każdego roku urodzenia występującego w danych\n",
        "#wylicz średnią stawkę godzinową posortuj od najmłodszych lat do najstarszych\n",
        "\n",
        "!head -5 (plik)\n",
        "schema_workers = StructType([StructField('imie', StringType()),\n",
        "                             StructField('nazwisko', StringType()),\n",
        "                             StructField('stawka_godzinowa_w dolarach', IntegerType()),\n",
        "                             StructField('rok_urodzenia', IntegerType())])\n",
        "df_workers = spark.read.csv(\"pracownicy.csv\", header=True, sep=\";\", schema=schema_workers).toPandas()\n",
        "df_workers.head(5)\n",
        "fragment=df_workers[['stawka_godzinowa_w dolarach', 'rok_urodzenia']]\n",
        "fragment_agg=fragment.groupby([\"rok_urodzenia\"]).mean('stawka_godzinowa_w dolarach')\n",
        "fragment_agg_sort=fragment_agg.sort_values(by='rok_urodzenia', ascending=False)\n",
        "fragment_agg_sort\n",
        "fragment_agg_sort.to_excel(\"wyniki_lata_stawka_sort.xlsx\")\n",
        "\n",
        "#zoptymalizowany kod\n",
        "df = spark.read.csv(\"pracownicy.csv\", header=True, sep=\";\", schema=schema_workers)\n",
        "x=(df.select(f.col(\"rok _urodzenia\"), f.col(\"stawka_godzinowa_w_dolarach\"))\n",
        ".groupBy(f.col(\"rok_urodzenia\")).mean(\"stawka_godzinowa_w_dolarach\")\n",
        ".orderBy(\"rok_urodzenia\", ascending=False))\n",
        "\n",
        "x.toPandas().to_excel(\"outpus_zad4.xlsx\")\n"
      ],
      "metadata": {
        "id": "KhWExb2oweyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Jakiego dnia ile było wejść a angielską wikipedię?\n",
        "#Rozwiąż na dwa sposoby (wykorzystując czyste RDD oraz Dataframes)\n",
        "#kod 1 (RDD)\n",
        "(pagecounts\n",
        "          .map(lambda x: (x[:8], x.split()[1], int(x.split()[3])))  # bierze date i liczbę\n",
        "          .filter(lambda x: x[1]==\"en\")\n",
        "          .map(lambda x: (x[0], x[2]))\n",
        "          .groupByKey()  # Grupuje po kluczu\n",
        "          .mapValues(sum)  # sumuje\n",
        "          .collect())  # wyświetla\n"
      ],
      "metadata": {
        "id": "4QeJTaPZyf1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(pagecounts\n",
        "  .map(lambda x: x.split(' '))\n",
        "  .filter(lambda x: x[1]=='en')\n",
        "  .map(lambda x: [x[0][:8], int(x[3])])\n",
        "  .reduceByKey(lambda x,y: x+y)\n",
        ").take(10)"
      ],
      "metadata": {
        "id": "FCjouXz1ynXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagecounts.take(10)"
      ],
      "metadata": {
        "id": "eDBe_gRayrC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kod 2 (Dataframes)\n",
        "schema_wiki = StructType([StructField('date', StringType()),\n",
        "                             StructField('lang', StringType()),\n",
        "                             StructField('title', StringType()),\n",
        "                             StructField('views', IntegerType()),\n",
        "                             StructField('code', IntegerType())])"
      ],
      "metadata": {
        "id": "ek0ZQ3EzyuG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagecount_df=spark.read.csv('pagecounts', sep=' ', schema=schema_wiki)"
      ],
      "metadata": {
        "id": "c5QuL209ywpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagecount_df.show(2)"
      ],
      "metadata": {
        "id": "ZMSby923yzTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagecount_df.printSchema()"
      ],
      "metadata": {
        "id": "aMVVG1-Iy4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagecount_df.createOrReplaceTempView(\"pagecount_df\")"
      ],
      "metadata": {
        "id": "xT_AhE4Yy5OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_do_wycinania_znakow(x):\n",
        "  return x[:8]"
      ],
      "metadata": {
        "id": "ZXgtA3kCy78Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "udf_do_wycinania_znakow=udf(f_do_wycinania_znakow, StringType())"
      ],
      "metadata": {
        "id": "q0bXVHBxy--S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(pagecount_df\n",
        "  .filter(f.col('lang')==\"en\")\n",
        "  .select(f.col('date'), f.col('views'))\n",
        "  .withColumn('date', udf_do_wycinania_roku(f.col('date')))\n",
        "  .groupBy('date')\n",
        "  .sum('views')\n",
        ").show()"
      ],
      "metadata": {
        "id": "17SgO9CZzBSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" SELECT SUBSTR(date, 1, 8) AS DATE, SUM(views) AS SUM_VIEW_COUNT\n",
        "            FROM pagecount_df\n",
        "            WHERE lang = 'en'\n",
        "            GROUP BY 1\"\"\").show()"
      ],
      "metadata": {
        "id": "KvQZ9fPLzEN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wczytaj wiki_sample.parquet (moodle) Odpowiedz na pytanie którzy użytkownicy\n",
        "#edytowali najwięcej stron na wikipedii (top 5)?Wykonaj zadanie na dwa sposoby\n",
        "# (Dataframe SQL oraz Dataframe programmatic API)"
      ],
      "metadata": {
        "id": "5l2eXVh5zLid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "!unzip wiki_sample.parquet\n",
        "df_wiki = spark.read.parquet(\"wiki_sample.parquet\")\n",
        "\n",
        "df_wiki.show()\n",
        "df_wiki.printSchema()\n",
        "\n",
        "#Data Frame programatic API\n",
        " (df_wiki\n",
        "  .filter(f.col('username')!=\"\")\n",
        "  .groupBy('username')\n",
        "  .count()\n",
        "  .sort('count', ascending=False)\n",
        ").show(5)\n",
        "\n",
        "#SQL\n",
        "df_wiki.createOrReplaceTempView(\"wiki_table\")\n",
        "\n",
        "top5_users = spark.sql(\"\"\"\n",
        "    SELECT username, COUNT(*) AS count\n",
        "    FROM wiki_table\n",
        "    WHERE username != \"\"\n",
        "    GROUP BY username\n",
        "    ORDER BY count DESC\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "top5_users.show()"
      ],
      "metadata": {
        "id": "zc7zStvwzYzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wczytaj plik bonuses.csv stwórz tabelę o kolumnach employee_id,\n",
        "#year, max_quarter_bonus, min_quarter_bonus, total_bonus,\n",
        "#total_bonus_previous_year, total_bonus_2_years_ago\n",
        "\n",
        "bonuses = spark.read.csv(\"bonuses.csv\", header=True, sep=';', inferSchema=True)\n",
        "\n",
        "bonuses.show()\n",
        "\n",
        "bonuses_agg = (bonuses\n",
        "               .groupBy([\"employee_id\", \"year\"]) \\\n",
        "          .agg(f.min('bonus').alias(\"MIN\"),\n",
        "         f.max('bonus').alias(\"MAX\"),\n",
        "         f.sum('bonus').alias(\"total_bonus\"))) \\\n",
        "         .orderBy(\"employee_id\", \"year\")\n",
        "\n",
        "bonuses_agg.show()\n",
        "\n",
        "# Definiowanie okna do obliczenia bonusów z poprzednich lat\n",
        "window_quarter = Window.partitionBy(\"employee_id\").orderBy(\"quarter\")\n",
        "\n",
        "# Dodawanie kolumn z bonusami z poprzednich lat\n",
        "result_df = bonuses.withColumn(\n",
        "    \"min_quarter_bonus\", f.min('bonus').over(window_quarter)\n",
        ").withColumn(\n",
        "    \"max_quarter_bonus\", f.max('bonus').over(window_quarter)\n",
        ")\n",
        "result_df.show()\n",
        "\n",
        "# Definiowanie okna do obliczenia bonusów z poprzednich lat\n",
        "window_spec = Window.partitionBy(\"employee_id\").orderBy(\"year\")\n",
        "\n",
        "# Dodawanie kolumn z bonusami z poprzednich lat\n",
        "result_df2 = result_df.withColumn(\n",
        "    \"total_bonus\", f.sum(\"bonus\", 1).over(window_spec))\n",
        "# ).withColumn(\n",
        "#     \"total_bonus_2_years_ago\",\n",
        "#     lag(\"total_bonus\", 2).over(window_spec)\n",
        "\n",
        "\n",
        "result_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QwzXUM-BzcP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uczenie maszynowe"
      ],
      "metadata": {
        "id": "BoQMJT7U0YTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do tego samego zbioru danych stworz model klasyfikacji oparty na Random\n",
        "#Forest (https://spark.apache.org/docs/3.1.1/api/python/reference/api/\n",
        "#pyspark.ml.classification.RandomForestClassifier.html). Postaraj się\n",
        "#zrobić go w formie \"pipeline\"."
      ],
      "metadata": {
        "id": "5p8ezvHi0Ozt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip"
      ],
      "metadata": {
        "id": "oQDESpXR08mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv(\"train.csv\", header=True, inferSchema=True, sep=\",\")"
      ],
      "metadata": {
        "id": "SyAx9Y2x1BcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "id": "1FyMWxZX1Bgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train, test) = data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "OkMbz7p51HXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[x for x in data.columns if x not in ['Id', 'Cover_Type']], outputCol='features')"
      ],
      "metadata": {
        "id": "2vX1MDBu1Kf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "labelIndexer = StringIndexer(inputCol='Cover_Type', outputCol='label').fit(data)"
      ],
      "metadata": {
        "id": "xlgUCz0g1Ooc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(numTrees=15, maxDepth=30, seed=42, featuresCol='features', labelCol='label')"
      ],
      "metadata": {
        "id": "9OVSrF1W1VS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, labelIndexer, rf])\n",
        "model_pipeline = pipeline.fit(train)"
      ],
      "metadata": {
        "id": "Vi2_omyv1X1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_pipeline.transform(test)"
      ],
      "metadata": {
        "id": "WkKsZPfR1d1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName='accuracy')"
      ],
      "metadata": {
        "id": "FB_wmAAR1gep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "yzR5pM8J1jaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "id": "ckhaWOaT1l6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IndexToString"
      ],
      "metadata": {
        "id": "pn5JhT3p1n1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelIndexer.labels"
      ],
      "metadata": {
        "id": "wxWRvbpO1r03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelConverter=IndexToString(inputCol='prediction', outputCol=\"Cover_Type_Predicted\", labels=labelIndexer.labels)"
      ],
      "metadata": {
        "id": "167mJZ2k1sY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Zaimplementuj hyperparameter tuning dla drzew decyzyjnych. Parametry:\n",
        "#impurity, [\"entropy\"]; maxDepth, [10, 30]; maxBins, [30, 300, 100];"
      ],
      "metadata": {
        "id": "6503FWTV12TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train, test) = data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "dj1Py3wl15Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "MAUHalOG1-mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(seed=42)"
      ],
      "metadata": {
        "id": "sZN88aIU2BRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "  .addGrid(dt.maxDepth, [10, 30])\n",
        "  .addGrid(dt.maxBins, [30, 300, 100])\n",
        "  .addGrid(dt.impurity, ['entropy'])\n",
        "  .build())\n"
      ],
      "metadata": {
        "id": "guBtW-u_2D_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[assembler, labelIndexer, dt, labelConverter])"
      ],
      "metadata": {
        "id": "SgDdScKy2F40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3) # use 10 folds in practice"
      ],
      "metadata": {
        "id": "kcwWHg9q2HuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel = crossval.fit(train)"
      ],
      "metadata": {
        "id": "FFHMOe4k2J13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [{p.name: v for p, v in m.items()} for m in cvModel.getEstimatorParamMaps()]\n",
        "params"
      ],
      "metadata": {
        "id": "BoRleAab2LXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel.avgMetrics"
      ],
      "metadata": {
        "id": "S5QBC3UY2NGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_index = cvModel.avgMetrics.index(max(cvModel.avgMetrics))\n",
        "best_param_map = params[best_index]\n",
        "best_param_map"
      ],
      "metadata": {
        "id": "x7MGDLRL2QlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cvModel.bestModel"
      ],
      "metadata": {
        "id": "hWId7MOz2Sjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model"
      ],
      "metadata": {
        "id": "uTPwE8v_2UYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = best_model.transform(test)"
      ],
      "metadata": {
        "id": "RqaWqINF2WnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test = evaluator.evaluate(predictions_test)"
      ],
      "metadata": {
        "id": "yv5_MX8W2Yy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy test: {accuracy_test}\")"
      ],
      "metadata": {
        "id": "oHGOwVYu2atF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regresja liniowa w Sparku - ile będzie rzutów na koniec 2022? Użyj Regresji Logistycznej"
      ],
      "metadata": {
        "id": "VedsTsZe2chx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Prepare the training data\n",
        "vector_assembler = VectorAssembler(inputCols=['yr'], outputCol='features')\n",
        "training = vector_assembler.transform(fga_py) \\\n",
        "                          .withColumn('label', col('fg3a_p36m'))\n",
        "\n",
        "training.show(5)"
      ],
      "metadata": {
        "id": "hAtMh_KF2ozF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Train linear regression model\n",
        "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
        "lr_model = lr.fit(training)\n",
        "\n",
        "# Prepare the test data for the year 2022\n",
        "test_data = spark.createDataFrame([(2022,)], [\"yr\"])\n",
        "test_features = vector_assembler.transform(test_data)\n",
        "\n",
        "# Make predictions for 2022\n",
        "predictions = lr_model.transform(test_features)\n",
        "predictions.select('yr', 'prediction').show()"
      ],
      "metadata": {
        "id": "Pz7IpgKX2ts6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Binarizacja etykiety na podstawie mediany\n",
        "training = training.withColumn('label',\n",
        "    when(col('fg3a_p36m') < 0.5, 0).\n",
        "    when((col('fg3a_p36m') > 0.5) & (col('fg3a_p36m') <1.5), 1).\n",
        "    when((col('fg3a_p36m') >= 1.5) & (col('fg3a_p36m') < 2.5), 2).\n",
        "    when((col('fg3a_p36m') >= 2.5) & (col('fg3a_p36m') < 3.5), 3).\n",
        "    otherwise(4)\n",
        ")\n",
        "\n",
        "# model\n",
        "log_reg = LogisticRegression(featuresCol='features', labelCol='label', family='multinomial')\n",
        "log_reg_model = log_reg.fit(training)\n",
        "\n",
        "test_data = spark.createDataFrame([(2022,)], [\"yr\"])\n",
        "test_features = vector_assembler.transform(test_data)\n",
        "\n",
        "# Step 6: Make Predictions\n",
        "predictions = log_reg_model.transform(test_features)\n",
        "predictions.select('yr', 'probability', 'prediction').show()"
      ],
      "metadata": {
        "id": "JOnVNmQU2uek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}